{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "differential-recycling",
   "metadata": {},
   "source": [
    "# A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contrary-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brown-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1pexp(x):\n",
    "    z = torch.stack([x, torch.zeros_like(x)], axis=-1)\n",
    "    return torch.logsumexp(z, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-memory",
   "metadata": {},
   "source": [
    "## Binarized MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lasting-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (x>0.5).view(-1).to(torch.int8)),\n",
    "])\n",
    "\n",
    "ds = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(ds, batch_size=256, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-wings",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wicked-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dz, Dh, Ddata = 2, 500, 28**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(z):\n",
    "    \"\"\"\n",
    "    z: input tensor with shape (batch_size, Dz)\n",
    "    \"\"\"\n",
    "    return torch.sum(-0.5 * (torch.log(torch.tensor(2 * math.pi, device=DEVICE)) + z ** 2), dim=-1)  # shape: (batch_size,)\n",
    "\n",
    "def bernoulli_log_density(x, logit_means):\n",
    "    \"\"\"\n",
    "    x: Input tensor with shape (batch_size, 784)\n",
    "    logit_means: Input tensor with shape (batch_size, 784)\n",
    "    \"\"\"\n",
    "    b = x * 2 - 1\n",
    "    return torch.sum(-log1pexp(-b * logit_means), dim=-1)  # shape: (batch_size,)\n",
    "\n",
    "\n",
    "decoder = nn.Sequential(nn.Linear(Dz, Dh),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(Dh, Ddata)).to(DEVICE)\n",
    "\n",
    "\n",
    "def log_likelihood(x,z):\n",
    "    return bernoulli_log_density(x, decoder(z))\n",
    "\n",
    "\n",
    "def joint_log_density(x,z):\n",
    "    return log_likelihood(x,z) + log_prior(z)  # shape: (batch_size, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-whole",
   "metadata": {},
   "source": [
    "## Amortized Approximate Inference with Learned Variational Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prerequisite-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_q(z, mu, log_sigma):\n",
    "    \"\"\"\n",
    "    z: tensor with shape (batch_size, Dz)\n",
    "    mu: tensor with shape (batch_size, Dz)\n",
    "    log_sigma: tensor with shape (batch_size, Dz)\n",
    "    \"\"\"\n",
    "    \n",
    "    return torch.sum(\n",
    "        -0.5 * torch.log(torch.tensor(2 * math.pi, device=DEVICE)) - \n",
    "        log_sigma - 0.5 * ((z - mu) / torch.exp(log_sigma)) ** 2, dim=1)  # shape: (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stylish-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(nn.Linear(Ddata, Dh),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(Dh, 2 * Dz)).to(DEVICE)\n",
    "\n",
    "\n",
    "def elbo(x):\n",
    "    batch_size = x.size(0)\n",
    "    enc_out = encoder(x)\n",
    "    mu, log_sigma = enc_out[..., :Dz], enc_out[..., Dz:]\n",
    "    z = torch.randn_like(log_sigma) * torch.exp(log_sigma) + mu\n",
    "    joint_ll = joint_log_density(x, z)\n",
    "    log_q_z = log_q(z, mu, log_sigma)\n",
    "    elbo_estimate = torch.mean(joint_ll - log_q_z)  # scalar\n",
    "    \n",
    "    return elbo_estimate\n",
    "\n",
    "\n",
    "def loss_fn(x):\n",
    "    return -elbo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-desktop",
   "metadata": {},
   "source": [
    "## Optimize the model and amortized variational parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "respective-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(enc, dec, data, loss_fn, n_epochs=10, print_every=500):\n",
    "    optimizer = torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=1e-4, )\n",
    "        \n",
    "    itrs = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch}')\n",
    "        for X_batch, y in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(X_batch.to(DEVICE).float())\n",
    "            \n",
    "            if itrs % print_every == 0:\n",
    "                print(f'\\tIteration {itrs}, Loss {loss.item()}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            itrs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eight-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tIteration 0, Loss 552.8909912109375\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "\tIteration 500, Loss 193.457763671875\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "\tIteration 1000, Loss 190.4912872314453\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "\tIteration 1500, Loss 185.35568237304688\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "\tIteration 2000, Loss 177.88592529296875\n",
      "Epoch 9\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, data_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-chester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-local",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
